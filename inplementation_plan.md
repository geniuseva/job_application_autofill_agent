## ğŸ§  Job Application Autofill Agent â€“ Implementation Plan  
**Team:** CodeRAGers  
**Members:** Yin Hu, Guo Fan  
**Framework:** Python + [AutoGen](https://github.com/microsoft/autogen)  
**Optional Evaluation** [Arize Phoenix](https://github.com/Arize-ai/phoenix)

---

## ğŸ“ Recommended Repository Structure

```
job-autofill-agent/
â”œâ”€â”€ agents/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ scraper_agent.py         # Extract form fields from job URLs
â”‚   â”œâ”€â”€ mapping_agent.py         # Map scraped fields to user data
â”‚   â”œâ”€â”€ db_agent.py              # Interface for user data storage
â”‚   â”œâ”€â”€ autofill_agent.py        # Automate form filling via Playwright
â”‚   â””â”€â”€ orchestrator.py          # (Optional) Handles workflows
â”‚   â””â”€â”€ user_proxy_agent.py      # Handles user feedback injection
â”‚
â”œâ”€â”€ workflows/
â”‚   â”œâ”€â”€ orchestrator_worker.py   # Multi-agent orchestrator architecture
â”‚   â”œâ”€â”€ prompt_chaining.py       # Prompt chaining-based architecture
â”‚   â”œâ”€â”€ groupchat_hitl.py        # Orchestrator with user agent 
â”‚
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ users.json               # Mock user database
â”‚   â””â”€â”€ ground_truths.json       # Manual labels for accuracy eval
â”‚
â”œâ”€â”€ evaluation/
â”‚   â”œâ”€â”€ evaluator.py             # Evaluate time, token, accuracy
â”‚   â””â”€â”€ metrics_logger.py        # For structured results logging
â”‚
â”œâ”€â”€ utils/
â”‚   â”œâ”€â”€ form_utils.py            # HTML parsing, field ID extraction
â”‚   â”œâ”€â”€ logger.py                # Unified logging
â”‚   â””â”€â”€ config.py                # Configuration (token limits, retries)
â”‚
â”œâ”€â”€ tests/
â”‚   â”œâ”€â”€ test_scraper_agent.py
â”‚   â”œâ”€â”€ test_mapping_agent.py
â”‚   â””â”€â”€ test_autofill_agent.py
â”‚
â”œâ”€â”€ notebooks/                   # Exploratory or debugging notebooks
â”‚   â””â”€â”€ form_scraping_demo.ipynb
â”‚
â”œâ”€â”€ .env                         # API keys, secrets
â”œâ”€â”€ README.md
â”œâ”€â”€ requirements.txt
â””â”€â”€ main.py                      # Entry point for the orchestrator
```

---

## ğŸš€ Incremental Development Plan

### âœ… **Phase 1: Setup & Baseline Agents**
- [ ] Create repo structure with placeholders and stubs
- [ ] Implement ScraperAgent using Playwright
- [ ] Mock basic form page to test extraction
- [ ] Implement MappingAgent with simple field matching logic
- [ ] Mock user database (JSON-based)
- [ ] Implement DBAgent with CRUD and query support

### âœ… **Phase 2: End-to-End Autofill Pipeline**
- [ ] Develop FormAutofillAgent with Playwright actions (click, input, submit)
- [ ] Integrate all agents with basic sequential flow
- [ ] Create orchestration layer via `AutoGen` (GroupChat / GroupChatManager)
- [ ] Add CLI or minimal UI to input job URL and return filled review URL
- [ ] Insert hook for potential user feedback after MappingAgent(Optional)

### âœ… **Phase 3: Evaluation Module**
- [ ] Build `evaluator.py` to track:
  - Accuracy (using `ground_truths.json`)
  - Token count (via OpenAI usage API or `AutoGen` logging)
  - Time per agent (timestamp logs)
- [ ] Save evaluation results in a structured CSV/JSON for analysis

### ğŸ” **Phase 4: User Feedback Integration(Optional) **

#### 4.1 Introduce `UserProxyAgent`
- [ ] Create a `UserProxyAgent` that simulates user prompts and responses
- [ ] Customize behavior (auto-respond in dev mode or allow real-time input)
- [ ] Trigger only when:
  - Mapping is incomplete (e.g., missing phone number, resume)
  - Confidence is low (can use basic heuristic or flag)

#### 4.2 Add GroupChatManager for Workflow Coordination
- [ ] Wrap all agents (Scraper, Mapping, DB, Autofill, UserProxy) into a `GroupChatManager`
- [ ] Define rules:
   - If MappingAgent identifies a missing field â†’ ask `UserProxyAgent`
   - If all fields filled â†’ proceed to AutofillAgent

#### 4.3 Mock Inputs for Testing
- [ ] Provide CLI/JSON mock interactions from users (simulate user saying â€œMy phone number is...â€)
- [ ] Log all user-agent conversations for later review

### ğŸ” **Phase 5 (Optional): Integration with Arize Phoenix**
- [ ] Integrate `phoenix.trace()` to track LLM performance
- [ ] Visualize token cost, success rate, time, anomalies
- [ ] Compare versions (with/without optimization)
- [ ] Add **HITL-enabled vs. fully automated workflows** to benchmark:

| Setup                  | Accuracy â†‘ | Token Cost â†‘ | Time â†‘ |
|------------------------|------------|---------------|--------|
| Fully Automated        | Medium     | Low           | Fast   |
| HITL via UserProxyAgent| High       | Medium        | Medium |

## ğŸ“Š Evaluation Matrix

| Metric          | Tools Used                    | Goal                             |
|----------------|-------------------------------|----------------------------------|
| Accuracy        | Manual labels, `difflib`      | Match filled vs ground truth     |
| Token Usage     | `AutoGen` logs, OpenAI API    | Optimize by caching, prompt ref. |
| Time Taken      | `time.perf_counter()` logs    | Lower with workflow optimizations|

---

